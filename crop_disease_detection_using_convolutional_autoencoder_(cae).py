# -*- coding: utf-8 -*-
"""Crop_Disease_Detection_Using_Convolutional_Autoencoder_(CAE).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kiFE5dzTgqopnxYsjkdzfD0KndoCNlQk
"""

from google.colab import drive
drive.mount('/content/drive')

import tensorflow as tf
from tensorflow.keras.layers import (
    Conv2D, MaxPooling2D, Dense, Dropout,
    BatchNormalization, Input, GRU, Reshape
)
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing import image_dataset_from_directory
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau

import numpy as np
import os
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")

# Configuration constants
BASE_DIR = '/content/drive/MyDrive/PlantVillage_Splits'
TRAIN_DIR = os.path.join(BASE_DIR, 'train')
VAL_DIR = os.path.join(BASE_DIR, 'val')
TEST_DIR = os.path.join(BASE_DIR, 'test')

# Hyperparameters
IMG_SIZE = (128, 128)
BATCH_SIZE = 32
EPOCHS = 20
LEARNING_RATE = 1e-4
SEED = 42

# Load datasets
train_ds = image_dataset_from_directory(
    TRAIN_DIR, image_size=IMG_SIZE, batch_size=BATCH_SIZE, 
    label_mode='categorical', seed=SEED
)

val_ds = image_dataset_from_directory(
    VAL_DIR, image_size=IMG_SIZE, batch_size=BATCH_SIZE, 
    label_mode='categorical', seed=SEED
)

test_ds = image_dataset_from_directory(
    TEST_DIR, image_size=IMG_SIZE, batch_size=BATCH_SIZE, 
    label_mode='categorical', shuffle=False
)

# Extract class information
class_names = train_ds.class_names
NUM_CLASSES = len(class_names)
print(f"Detected Classes: {class_names}")
print(f"Total Classes: {NUM_CLASSES}")

# Data augmentation configuration
ROTATION_FACTOR = 0.2
ZOOM_FACTOR = 0.2
CONTRAST_FACTOR = 0.2
TRANSLATION_FACTOR = 0.1

data_augmentation = tf.keras.Sequential([
    tf.keras.layers.Rescaling(1./255),
    tf.keras.layers.RandomFlip("horizontal"),
    tf.keras.layers.RandomRotation(ROTATION_FACTOR),
    tf.keras.layers.RandomZoom(ZOOM_FACTOR),
    tf.keras.layers.RandomContrast(CONTRAST_FACTOR),
    tf.keras.layers.RandomTranslation(TRANSLATION_FACTOR, TRANSLATION_FACTOR)
])

# Apply preprocessing
def normalize_image(x, y):
    return x / 255.0, y

train_ds = train_ds.map(lambda x, y: (data_augmentation(x), y))
val_ds = val_ds.map(normalize_image)
test_ds = test_ds.map(normalize_image)

train_ds = train_ds.prefetch(tf.data.AUTOTUNE)
val_ds   = val_ds.prefetch(tf.data.AUTOTUNE)
test_ds  = test_ds.prefetch(tf.data.AUTOTUNE)

inputs = Input(shape=(128, 128, 3))

# CNN Feature Extraction
# Define filter sizes as constants
FILTERS_1 = 32
FILTERS_2 = 64
FILTERS_3 = 128
FILTERS_4 = 256
KERNEL_SIZE = 3
POOL_SIZE = 2

x = Conv2D(FILTERS_1, KERNEL_SIZE, activation='relu', padding='same')(inputs)
x = BatchNormalization()(x)
x = MaxPooling2D(POOL_SIZE)(x)

x = Conv2D(FILTERS_2, KERNEL_SIZE, activation='relu', padding='same')(x)
x = BatchNormalization()(x)
x = MaxPooling2D(POOL_SIZE)(x)

x = Conv2D(FILTERS_3, KERNEL_SIZE, activation='relu', padding='same')(x)
x = BatchNormalization()(x)
x = MaxPooling2D(POOL_SIZE)(x)

x = Conv2D(FILTERS_4, KERNEL_SIZE, activation='relu', padding='same')(x)
x = BatchNormalization()(x)
x = MaxPooling2D(POOL_SIZE)(x)

# Reshape for RNN: (batch_size, timesteps, features)
TIMESTEPS = 8
FEATURE_DIM = 256 * 8
x = Reshape((TIMESTEPS, FEATURE_DIM))(x)

# RNN Layer (GRU)
GRU_UNITS = 256
x = GRU(GRU_UNITS, return_sequences=False)(x)

# Dense Layers for classification
DENSE_UNITS = 256
DROPOUT_RATE = 0.5
x = Dense(DENSE_UNITS, activation='relu')(x)
x = Dropout(DROPOUT_RATE)(x)
outputs = Dense(NUM_CLASSES, activation='softmax')(x)

model = Model(inputs, outputs)

# Compile model
optimizer = Adam(learning_rate=LEARNING_RATE)
loss_function = 'categorical_crossentropy'
metrics_list = ['accuracy']

model.compile(
    optimizer=optimizer,
    loss=loss_function,
    metrics=metrics_list
)

model.summary()

# Training callbacks configuration
EARLY_STOP_PATIENCE = 5
LR_REDUCTION_FACTOR = 0.5
LR_REDUCTION_PATIENCE = 3
CHECKPOINT_FILENAME = 'best_crnn_model.keras'

early_stop = EarlyStopping(
    monitor='val_accuracy', 
    patience=EARLY_STOP_PATIENCE, 
    restore_best_weights=True
)
checkpoint = ModelCheckpoint(
    CHECKPOINT_FILENAME, 
    monitor='val_accuracy', 
    save_best_only=True
)
reduce_lr = ReduceLROnPlateau(
    monitor='val_loss', 
    factor=LR_REDUCTION_FACTOR, 
    patience=LR_REDUCTION_PATIENCE
)

history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=EPOCHS,
    callbacks=[early_stop, checkpoint, reduce_lr]
)

# Save model
MODEL_PATH = "/content/drive/MyDrive/PlantVillage_CRNN_Model.keras"
model.save(MODEL_PATH)
print(f"Model saved successfully to {MODEL_PATH}")

# Evaluate model
test_loss, test_acc = model.evaluate(test_ds, verbose=1)
print(f"Test Loss: {test_loss:.4f}")
print(f"Test Accuracy: {test_acc*100:.2f}%")

# Generate predictions for confusion matrix
y_true, y_pred = [], []

for batch_x, batch_y in test_ds:
    batch_preds = model.predict(batch_x, verbose=0)
    y_true.extend(np.argmax(batch_y.numpy(), axis=1))
    y_pred.extend(np.argmax(batch_preds, axis=1))

# Generate and visualize confusion matrix
cm = tf.math.confusion_matrix(y_true, y_pred)

FIG_SIZE = (12, 10)
plt.figure(figsize=FIG_SIZE)
sns.heatmap(
    cm, 
    xticklabels=class_names, 
    yticklabels=class_names, 
    cmap="Blues",
    annot=True,
    fmt='d'
)
plt.title("Confusion Matrix - Plant Disease Classification")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.tight_layout()
plt.show()

# =============================
# SINGLE IMAGE PREDICTION SECTION
# =============================
# Note: This section is for inference on a single image
# Remove duplicate imports if running in same session

PREDICTION_MODEL_PATH = "/content/drive/MyDrive/PlantVillage_CRNN_Model.keras"
IMAGE_PATH = "/content/drive/MyDrive/test_image/test3.png"
PREDICTION_IMG_SIZE = (128, 128)

# =============================
# LOAD MODEL
# =============================
prediction_model = tf.keras.models.load_model(PREDICTION_MODEL_PATH)
print(f"Model loaded successfully from {PREDICTION_MODEL_PATH}")

# =============================
# LOAD CLASS NAMES (IMPORTANT)
# =============================
PREDICTION_TRAIN_DIR = '/content/drive/MyDrive/PlantVillage_Splits/train'
prediction_class_names = sorted(os.listdir(PREDICTION_TRAIN_DIR))
print(f"Classes: {prediction_class_names}")

# =============================
# LOAD & PREPROCESS IMAGE
# =============================
def preprocess_image(image_path, target_size):
    img = tf.keras.preprocessing.image.load_img(image_path, target_size=target_size)
    img_arr = tf.keras.preprocessing.image.img_to_array(img) / 255.0
    img_arr = np.expand_dims(img_arr, axis=0)
    return img, img_arr

img, img_arr = preprocess_image(IMAGE_PATH, PREDICTION_IMG_SIZE)

# =============================
# PREDICTION
# =============================
pred = prediction_model.predict(img_arr, verbose=0)
idx = np.argmax(pred[0])

predicted_class = prediction_class_names[idx]
confidence = np.max(pred[0]) * 100

def get_disease_status(class_name):
    return "HEALTHY" if "healthy" in class_name.lower() else "DISEASED"

status = get_disease_status(predicted_class)

# =============================
# SHOW RESULT
# =============================
plt.imshow(img)
plt.axis("off")
plt.title(f"{predicted_class}\n{status} ({confidence:.2f}%)")
plt.show()

print("Predicted Class :", predicted_class)
print("Status          :", status)
print(f"Confidence      : {confidence:.2f}%")