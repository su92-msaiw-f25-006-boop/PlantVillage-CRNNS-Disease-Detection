# -*- coding: utf-8 -*-
"""Crop_Disease_Detection_Using_Convolutional_Autoencoder_(CAE).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kiFE5dzTgqopnxYsjkdzfD0KndoCNlQk
"""

from google.colab import drive
drive.mount('/content/drive')

import tensorflow as tf
from tensorflow.keras.layers import (
    Conv2D, MaxPooling2D, Dense, Dropout,
    BatchNormalization, Input, GRU, Reshape
)
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing import image_dataset_from_directory
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau

import numpy as np
import os
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")

# Configuration constants
def get_dataset_paths(base_dir):
    """Get train, validation, and test directory paths."""
    return {
        'train': os.path.join(base_dir, 'train'),
        'val': os.path.join(base_dir, 'val'),
        'test': os.path.join(base_dir, 'test')
    }

BASE_DIR = '/content/drive/MyDrive/PlantVillage_Splits'
paths = get_dataset_paths(BASE_DIR)
TRAIN_DIR = paths['train']
VAL_DIR = paths['val']
TEST_DIR = paths['test']

# Hyperparameters
IMG_SIZE = (128, 128)
BATCH_SIZE = 32
EPOCHS = 20
LEARNING_RATE = 1e-4
SEED = 42

# Load datasets
def load_dataset(directory, image_size, batch_size, label_mode='categorical', seed=None, shuffle=True):
    """Load image dataset from directory."""
    return image_dataset_from_directory(
        directory, 
        image_size=image_size, 
        batch_size=batch_size, 
        label_mode=label_mode, 
        seed=seed,
        shuffle=shuffle
    )

train_ds = load_dataset(TRAIN_DIR, IMG_SIZE, BATCH_SIZE, seed=SEED)
val_ds = load_dataset(VAL_DIR, IMG_SIZE, BATCH_SIZE, seed=SEED)
test_ds = load_dataset(TEST_DIR, IMG_SIZE, BATCH_SIZE, shuffle=False)

# Extract class information
def get_class_info(dataset):
    """Extract class names and count from dataset."""
    class_names = dataset.class_names
    num_classes = len(class_names)
    return class_names, num_classes

class_names, NUM_CLASSES = get_class_info(train_ds)
print(f"Detected Classes: {class_names}")
print(f"Total Classes: {NUM_CLASSES}")

# Data augmentation configuration
def create_augmentation_pipeline(rotation=0.2, zoom=0.2, contrast=0.2, translation=0.1):
    """Create data augmentation pipeline."""
    return tf.keras.Sequential([
        tf.keras.layers.Rescaling(1./255),
        tf.keras.layers.RandomFlip("horizontal"),
        tf.keras.layers.RandomRotation(rotation),
        tf.keras.layers.RandomZoom(zoom),
        tf.keras.layers.RandomContrast(contrast),
        tf.keras.layers.RandomTranslation(translation, translation)
    ])

ROTATION_FACTOR = 0.2
ZOOM_FACTOR = 0.2
CONTRAST_FACTOR = 0.2
TRANSLATION_FACTOR = 0.1

data_augmentation = create_augmentation_pipeline(
    ROTATION_FACTOR, ZOOM_FACTOR, CONTRAST_FACTOR, TRANSLATION_FACTOR
)

# Apply preprocessing
def normalize_image(x, y):
    return x / 255.0, y

train_ds = train_ds.map(lambda x, y: (data_augmentation(x), y))
val_ds = val_ds.map(normalize_image)
test_ds = test_ds.map(normalize_image)

def optimize_dataset(dataset):
    """Optimize dataset with prefetching."""
    return dataset.prefetch(tf.data.AUTOTUNE)

train_ds = optimize_dataset(train_ds)
val_ds = optimize_dataset(val_ds)
test_ds = optimize_dataset(test_ds)

# Model input configuration
INPUT_SHAPE = (128, 128, 3)
inputs = Input(shape=INPUT_SHAPE)

# CNN Feature Extraction
# Define filter sizes as constants
FILTERS_1 = 32
FILTERS_2 = 64
FILTERS_3 = 128
FILTERS_4 = 256
KERNEL_SIZE = 3
POOL_SIZE = 2

x = Conv2D(FILTERS_1, KERNEL_SIZE, activation='relu', padding='same')(inputs)
x = BatchNormalization()(x)
x = MaxPooling2D(POOL_SIZE)(x)

x = Conv2D(FILTERS_2, KERNEL_SIZE, activation='relu', padding='same')(x)
x = BatchNormalization()(x)
x = MaxPooling2D(POOL_SIZE)(x)

x = Conv2D(FILTERS_3, KERNEL_SIZE, activation='relu', padding='same')(x)
x = BatchNormalization()(x)
x = MaxPooling2D(POOL_SIZE)(x)

x = Conv2D(FILTERS_4, KERNEL_SIZE, activation='relu', padding='same')(x)
x = BatchNormalization()(x)
x = MaxPooling2D(POOL_SIZE)(x)

# Reshape for RNN: (batch_size, timesteps, features)
def calculate_feature_dim(height, width, filters):
    """Calculate feature dimension for reshape layer."""
    return height * width * filters

TIMESTEPS = 8
FEATURE_DIM = calculate_feature_dim(8, 8, FILTERS_4)
x = Reshape((TIMESTEPS, FEATURE_DIM))(x)

# RNN Layer (GRU)
GRU_UNITS = 256
x = GRU(GRU_UNITS, return_sequences=False)(x)

# Dense Layers for classification
DENSE_UNITS = 256
DROPOUT_RATE = 0.5
x = Dense(DENSE_UNITS, activation='relu')(x)
x = Dropout(DROPOUT_RATE)(x)
outputs = Dense(NUM_CLASSES, activation='softmax')(x)

def build_model(input_shape, num_classes, gru_units=256, dense_units=256, dropout_rate=0.5):
    """Build CRNN model architecture."""
    inputs = Input(shape=input_shape)
    
    # CNN Feature Extraction
    FILTERS_1 = 32
    FILTERS_2 = 64
    FILTERS_3 = 128
    FILTERS_4 = 256
    KERNEL_SIZE = 3
    POOL_SIZE = 2
    
    x = Conv2D(FILTERS_1, KERNEL_SIZE, activation='relu', padding='same')(inputs)
    x = BatchNormalization()(x)
    x = MaxPooling2D(POOL_SIZE)(x)
    
    x = Conv2D(FILTERS_2, KERNEL_SIZE, activation='relu', padding='same')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D(POOL_SIZE)(x)
    
    x = Conv2D(FILTERS_3, KERNEL_SIZE, activation='relu', padding='same')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D(POOL_SIZE)(x)
    
    x = Conv2D(FILTERS_4, KERNEL_SIZE, activation='relu', padding='same')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D(POOL_SIZE)(x)
    
    # Reshape for RNN
    TIMESTEPS = 8
    FEATURE_DIM = calculate_feature_dim(8, 8, FILTERS_4)
    x = Reshape((TIMESTEPS, FEATURE_DIM))(x)
    
    # RNN Layer
    x = GRU(gru_units, return_sequences=False)(x)
    
    # Classifier
    x = Dense(dense_units, activation='relu')(x)
    x = Dropout(dropout_rate)(x)
    outputs = Dense(num_classes, activation='softmax')(x)
    
    return Model(inputs, outputs)

model = build_model(INPUT_SHAPE, NUM_CLASSES, GRU_UNITS, DENSE_UNITS, DROPOUT_RATE)

# Compile model
optimizer = Adam(learning_rate=LEARNING_RATE)
loss_function = 'categorical_crossentropy'
metrics_list = ['accuracy']

model.compile(
    optimizer=optimizer,
    loss=loss_function,
    metrics=metrics_list
)

model.summary()

# Training callbacks configuration
def create_callbacks(early_stop_patience=5, lr_factor=0.5, lr_patience=3, checkpoint_file='best_crnn_model.keras'):
    """Create training callbacks."""
    early_stop = EarlyStopping(
        monitor='val_accuracy', 
        patience=early_stop_patience, 
        restore_best_weights=True
    )
    checkpoint = ModelCheckpoint(
        checkpoint_file, 
        monitor='val_accuracy', 
        save_best_only=True
    )
    reduce_lr = ReduceLROnPlateau(
        monitor='val_loss', 
        factor=lr_factor, 
        patience=lr_patience
    )
    return [early_stop, checkpoint, reduce_lr]

EARLY_STOP_PATIENCE = 5
LR_REDUCTION_FACTOR = 0.5
LR_REDUCTION_PATIENCE = 3
CHECKPOINT_FILENAME = 'best_crnn_model.keras'

callbacks_list = create_callbacks(
    EARLY_STOP_PATIENCE, 
    LR_REDUCTION_FACTOR, 
    LR_REDUCTION_PATIENCE, 
    CHECKPOINT_FILENAME
)

# Train model
callbacks_list = [early_stop, checkpoint, reduce_lr]

history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=EPOCHS,
    callbacks=callbacks_list,
    verbose=1
)

# Save model
MODEL_PATH = "/content/drive/MyDrive/PlantVillage_CRNN_Model.keras"
model.save(MODEL_PATH)
print(f"Model saved successfully to {MODEL_PATH}")

# Evaluate model
test_loss, test_acc = model.evaluate(test_ds, verbose=1)
print(f"Test Loss: {test_loss:.4f}")
print(f"Test Accuracy: {test_acc*100:.2f}%")

# Generate predictions for confusion matrix
def get_predictions(model, dataset):
    y_true, y_pred = [], []
    for batch_x, batch_y in dataset:
        batch_preds = model.predict(batch_x, verbose=0)
        y_true.extend(np.argmax(batch_y.numpy(), axis=1))
        y_pred.extend(np.argmax(batch_preds, axis=1))
    return y_true, y_pred

y_true, y_pred = get_predictions(model, test_ds)

# Generate and visualize confusion matrix
def plot_confusion_matrix(y_true, y_pred, class_names, title="Confusion Matrix"):
    cm = tf.math.confusion_matrix(y_true, y_pred)
    FIG_SIZE = (12, 10)
    plt.figure(figsize=FIG_SIZE)
    sns.heatmap(
        cm, 
        xticklabels=class_names, 
        yticklabels=class_names, 
        cmap="Blues",
        annot=True,
        fmt='d'
    )
    plt.title(f"{title} - Plant Disease Classification")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.tight_layout()
    plt.show()
    return cm

cm = plot_confusion_matrix(y_true, y_pred, class_names)

# =============================
# SINGLE IMAGE PREDICTION SECTION
# =============================
# Note: This section is for inference on a single image
# Remove duplicate imports if running in same session

PREDICTION_MODEL_PATH = "/content/drive/MyDrive/PlantVillage_CRNN_Model.keras"
IMAGE_PATH = "/content/drive/MyDrive/test_image/test3.png"
PREDICTION_IMG_SIZE = (128, 128)

# =============================
# LOAD MODEL
# =============================
prediction_model = tf.keras.models.load_model(PREDICTION_MODEL_PATH)
print(f"Model loaded successfully from {PREDICTION_MODEL_PATH}")

# =============================
# LOAD CLASS NAMES (IMPORTANT)
# =============================
PREDICTION_TRAIN_DIR = '/content/drive/MyDrive/PlantVillage_Splits/train'
prediction_class_names = sorted(os.listdir(PREDICTION_TRAIN_DIR))
print(f"Classes: {prediction_class_names}")

# =============================
# LOAD & PREPROCESS IMAGE
# =============================
def preprocess_image(image_path, target_size):
    img = tf.keras.preprocessing.image.load_img(image_path, target_size=target_size)
    img_arr = tf.keras.preprocessing.image.img_to_array(img) / 255.0
    img_arr = np.expand_dims(img_arr, axis=0)
    return img, img_arr

img, img_arr = preprocess_image(IMAGE_PATH, PREDICTION_IMG_SIZE)

# =============================
# PREDICTION
# =============================
pred = prediction_model.predict(img_arr, verbose=0)
idx = np.argmax(pred[0])

predicted_class = prediction_class_names[idx]
confidence = np.max(pred[0]) * 100

def get_disease_status(class_name):
    """Determine if plant is healthy or diseased based on class name."""
    return "HEALTHY" if "healthy" in class_name.lower() else "DISEASED"

def format_prediction_output(predicted_class, status, confidence):
    """Format prediction results for display."""
    return {
        'class': predicted_class,
        'status': status,
        'confidence': confidence
    }

status = get_disease_status(predicted_class)
prediction_result = format_prediction_output(predicted_class, status, confidence)

# =============================
# SHOW RESULT
# =============================
plt.imshow(img)
plt.axis("off")
plt.title(f"{predicted_class}\n{status} ({confidence:.2f}%)")
plt.show()

print("Predicted Class :", predicted_class)
print("Status          :", status)
print(f"Confidence      : {confidence:.2f}%")