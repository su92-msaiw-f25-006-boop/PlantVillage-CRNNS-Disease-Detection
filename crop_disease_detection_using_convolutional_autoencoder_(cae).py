# -*- coding: utf-8 -*-
"""Crop_Disease_Detection_Using_Convolutional_Autoencoder_(CAE).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kiFE5dzTgqopnxYsjkdzfD0KndoCNlQk
"""

from google.colab import drive
drive.mount('/content/drive')

import tensorflow as tf
from tensorflow.keras.layers import (
    Conv2D, MaxPooling2D, Dense, Dropout,
    Flatten, BatchNormalization, Input, LSTM, GRU, TimeDistributed
)
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing import image_dataset_from_directory
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau

import numpy as np
import os
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")

BASE_DIR = '/content/drive/MyDrive/PlantVillage_Splits'

TRAIN_DIR = os.path.join(BASE_DIR, 'train')
VAL_DIR   = os.path.join(BASE_DIR, 'val')
TEST_DIR  = os.path.join(BASE_DIR, 'test')

IMG_SIZE = (128, 128)
BATCH_SIZE = 32
EPOCHS = 20
LEARNING_RATE = 1e-4

train_ds = image_dataset_from_directory(
    TRAIN_DIR, image_size=IMG_SIZE, batch_size=BATCH_SIZE, label_mode='categorical', seed=42
)

val_ds = image_dataset_from_directory(
    VAL_DIR, image_size=IMG_SIZE, batch_size=BATCH_SIZE, label_mode='categorical', seed=42
)

test_ds = image_dataset_from_directory(
    TEST_DIR, image_size=IMG_SIZE, batch_size=BATCH_SIZE, label_mode='categorical', shuffle=False
)

class_names = train_ds.class_names
NUM_CLASSES = len(class_names)
print("Detected Classes:", class_names)
print("Total Classes:", NUM_CLASSES)

data_augmentation = tf.keras.Sequential([
    tf.keras.layers.Rescaling(1./255),
    tf.keras.layers.RandomFlip("horizontal"),
    tf.keras.layers.RandomRotation(0.2),
    tf.keras.layers.RandomZoom(0.2),
    tf.keras.layers.RandomContrast(0.2),
    tf.keras.layers.RandomTranslation(0.1, 0.1)
])

train_ds = train_ds.map(lambda x, y: (data_augmentation(x), y))
val_ds   = val_ds.map(lambda x, y: (x / 255.0, y))
test_ds  = test_ds.map(lambda x, y: (x / 255.0, y))

train_ds = train_ds.prefetch(tf.data.AUTOTUNE)
val_ds   = val_ds.prefetch(tf.data.AUTOTUNE)
test_ds  = test_ds.prefetch(tf.data.AUTOTUNE)

inputs = Input(shape=(128, 128, 3))

# CNN Feature Extraction
x = Conv2D(32, 3, activation='relu', padding='same')(inputs)
x = BatchNormalization()(x)
x = MaxPooling2D(2)(x)

x = Conv2D(64, 3, activation='relu', padding='same')(x)
x = BatchNormalization()(x)
x = MaxPooling2D(2)(x)

x = Conv2D(128, 3, activation='relu', padding='same')(x)
x = BatchNormalization()(x)
x = MaxPooling2D(2)(x)

x = Conv2D(256, 3, activation='relu', padding='same')(x)
x = BatchNormalization()(x)
x = MaxPooling2D(2)(x)

# Reshape for RNN: (batch_size, timesteps, features)
# Here we treat rows of feature maps as timesteps
x = tf.keras.layers.Reshape((8, 256*8))(x)  # (batch, timesteps=8, features=2048)

# RNN Layer (GRU)
x = GRU(256, return_sequences=False)(x)  # final output

# Dense Layers for classification
x = Dense(256, activation='relu')(x)
x = Dropout(0.5)(x)
outputs = Dense(NUM_CLASSES, activation='softmax')(x)

model = Model(inputs, outputs)

model.compile(
    optimizer=Adam(learning_rate=LEARNING_RATE),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

model.summary()

early_stop = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)
checkpoint = ModelCheckpoint('best_crnn_model.keras', monitor='val_accuracy', save_best_only=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)

history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=EPOCHS,
    callbacks=[early_stop, checkpoint, reduce_lr]
)

MODEL_PATH = "/content/drive/MyDrive/PlantVillage_CRNN_Model.keras"
model.save(MODEL_PATH)
print("‚úÖ Model saved successfully")

test_loss, test_acc = model.evaluate(test_ds)
print(f"üî• Test Accuracy: {test_acc*100:.2f}%")

y_true, y_pred = [], []

for x, y in test_ds:
    preds = model.predict(x)
    y_true.extend(np.argmax(y.numpy(), axis=1))
    y_pred.extend(np.argmax(preds, axis=1))

cm = tf.math.confusion_matrix(y_true, y_pred)

plt.figure(figsize=(12,10))
sns.heatmap(cm, xticklabels=class_names, yticklabels=class_names, cmap="Blues")
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

from google.colab import drive
drive.mount('/content/drive')

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import os

# =============================
# CONFIG
# =============================
MODEL_PATH = "/content/drive/MyDrive/PlantVillage_CRNN_Model.keras"
IMAGE_PATH = "/content/drive/MyDrive/test_image/test3.png"
IMG_SIZE = (128, 128)

# =============================
# LOAD MODEL
# =============================
model = tf.keras.models.load_model(MODEL_PATH)
print("‚úÖ Model loaded successfully")

# =============================
# LOAD CLASS NAMES (IMPORTANT)
# =============================
BASE_DIR = '/content/drive/MyDrive/PlantVillage_Splits/train'
class_names = sorted(os.listdir(BASE_DIR))
print("Classes:", class_names)

# =============================
# LOAD & PREPROCESS IMAGE
# =============================
img = tf.keras.preprocessing.image.load_img(
    IMAGE_PATH, target_size=IMG_SIZE
)

img_arr = tf.keras.preprocessing.image.img_to_array(img) / 255.0
img_arr = np.expand_dims(img_arr, axis=0)

# =============================
# PREDICTION
# =============================
pred = model.predict(img_arr)
idx = np.argmax(pred[0])

predicted_class = class_names[idx]
confidence = np.max(pred[0]) * 100

status = "HEALTHY ‚úÖ" if "healthy" in predicted_class.lower() else "DISEASED ‚ùå"

# =============================
# SHOW RESULT
# =============================
plt.imshow(img)
plt.axis("off")
plt.title(f"{predicted_class}\n{status} ({confidence:.2f}%)")
plt.show()

print("Predicted Class :", predicted_class)
print("Status          :", status)
print(f"Confidence      : {confidence:.2f}%")